{"step": 1, "data": {"train/loss": 1.371598482131958, "train/grad_norm": 22.445348739624023, "train/lr": 1.0000000000000001e-07, "train/mfu": 0.006220312052124726, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 2, "data": {"train/loss": 1.4290519952774048, "train/grad_norm": 22.00897216796875, "train/lr": 1.5000000000000002e-07, "train/mfu": 0.007227610492139833, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 3, "data": {"train/loss": 1.343455195426941, "train/grad_norm": 21.255949020385742, "train/lr": 2.0000000000000002e-07, "train/mfu": 0.00782493343780659, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 4, "data": {"train/loss": 1.3845702409744263, "train/grad_norm": 21.414926528930664, "train/lr": 2.5000000000000004e-07, "train/mfu": 0.00781792343267928, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 5, "data": {"train/loss": 1.4215142726898193, "train/grad_norm": 22.66004753112793, "train/lr": 3.0000000000000004e-07, "train/mfu": 0.00771340055667893, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 6, "data": {"train/loss": 1.3690203428268433, "train/grad_norm": 20.573436737060547, "train/lr": 3.5000000000000004e-07, "train/mfu": 0.007715903606327209, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 7, "data": {"train/loss": 1.3751790523529053, "train/grad_norm": 22.114246368408203, "train/lr": 4.0000000000000003e-07, "train/mfu": 0.0077280520135392, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 8, "data": {"train/loss": 1.3687165975570679, "train/grad_norm": 22.490177154541016, "train/lr": 4.5000000000000003e-07, "train/mfu": 0.007743627210950767, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 9, "data": {"train/loss": 1.335197925567627, "train/grad_norm": 22.094934463500977, "train/lr": 5.000000000000001e-07, "train/mfu": 0.007709565552820516, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 10, "data": {"train/loss": 1.4278473854064941, "train/grad_norm": 22.417814254760742, "train/lr": 5.5e-07, "train/mfu": 0.007739874056478405, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 11, "data": {"train/loss": 1.392138957977295, "train/grad_norm": 20.797468185424805, "train/lr": 6.000000000000001e-07, "train/mfu": 0.007468110967342828, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 12, "data": {"train/loss": 1.3754889965057373, "train/grad_norm": 21.013031005859375, "train/lr": 6.5e-07, "train/mfu": 0.00774495918762824, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 13, "data": {"train/loss": 1.331119179725647, "train/grad_norm": 18.949060440063477, "train/lr": 7.000000000000001e-07, "train/mfu": 0.007734096728483973, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 14, "data": {"train/loss": 1.3428359031677246, "train/grad_norm": 18.663000106811523, "train/lr": 7.5e-07, "train/mfu": 0.007771707563003998, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 15, "data": {"train/loss": 1.3224542140960693, "train/grad_norm": 19.127527236938477, "train/lr": 8.000000000000001e-07, "train/mfu": 0.007771541821242744, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 16, "data": {"train/loss": 1.2622463703155518, "train/grad_norm": 16.805416107177734, "train/lr": 8.500000000000001e-07, "train/mfu": 0.007755892377420072, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 17, "data": {"train/loss": 1.3393535614013672, "train/grad_norm": 18.037006378173828, "train/lr": 9.000000000000001e-07, "train/mfu": 0.007784665354059021, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 18, "data": {"train/loss": 1.2521414756774902, "train/grad_norm": 17.012104034423828, "train/lr": 9.500000000000001e-07, "train/mfu": 0.007732236557100918, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 19, "data": {"train/loss": 1.2113494873046875, "train/grad_norm": 17.511837005615234, "train/lr": 1.0000000000000002e-06, "train/mfu": 0.007647457597787445, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 20, "data": {"train/loss": 1.1725575923919678, "train/grad_norm": 16.1657657623291, "train/lr": 1.0500000000000001e-06, "train/mfu": 0.007821541135543448, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 21, "data": {"train/loss": 1.1781585216522217, "train/grad_norm": 16.379980087280273, "train/lr": 1.1e-06, "train/mfu": 0.007776133091420901, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 22, "data": {"train/loss": 1.1361271142959595, "train/grad_norm": 15.965834617614746, "train/lr": 1.1500000000000002e-06, "train/mfu": 0.00779093551468183, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 23, "data": {"train/loss": 1.1086628437042236, "train/grad_norm": 16.26612663269043, "train/lr": 1.2000000000000002e-06, "train/mfu": 0.007764634544252064, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 24, "data": {"train/loss": 1.1148662567138672, "train/grad_norm": 16.602497100830078, "train/lr": 1.25e-06, "train/mfu": 0.00784575992217654, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 25, "data": {"train/loss": 1.1307862997055054, "train/grad_norm": 19.71923065185547, "train/lr": 1.3e-06, "train/mfu": 0.007704835172311307, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 26, "data": {"train/loss": 0.8838002681732178, "train/grad_norm": 10.049785614013672, "train/lr": 1.3500000000000002e-06, "train/mfu": 0.007779163269100952, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 27, "data": {"train/loss": 0.8817644119262695, "train/grad_norm": 10.462372779846191, "train/lr": 1.4000000000000001e-06, "train/mfu": 0.007780670835401532, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 28, "data": {"train/loss": 0.8614932298660278, "train/grad_norm": 9.954288482666016, "train/lr": 1.45e-06, "train/mfu": 0.007734899351774067, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 29, "data": {"train/loss": 0.8537925481796265, "train/grad_norm": 9.659360885620117, "train/lr": 1.5e-06, "train/mfu": 0.007721997601561358, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 30, "data": {"train/loss": 0.83306884765625, "train/grad_norm": 9.901093482971191, "train/lr": 1.5500000000000002e-06, "train/mfu": 0.007813498058091766, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 31, "data": {"train/loss": 0.7727035284042358, "train/grad_norm": 8.785030364990234, "train/lr": 1.6000000000000001e-06, "train/mfu": 0.007777642404460222, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 32, "data": {"train/loss": 0.8114298582077026, "train/grad_norm": 8.4578218460083, "train/lr": 1.6500000000000003e-06, "train/mfu": 0.008299699862893039, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 33, "data": {"train/loss": 0.8292142748832703, "train/grad_norm": 8.33568286895752, "train/lr": 1.7000000000000002e-06, "train/mfu": 0.010087499101221808, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 34, "data": {"train/loss": 0.7348940372467041, "train/grad_norm": 7.89088249206543, "train/lr": 1.75e-06, "train/mfu": 0.009563770441210748, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 35, "data": {"train/loss": 0.7017204761505127, "train/grad_norm": 6.327545166015625, "train/lr": 1.8000000000000001e-06, "train/mfu": 0.008079105331985073, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 36, "data": {"train/loss": 0.6722939014434814, "train/grad_norm": 5.6477766036987305, "train/lr": 1.85e-06, "train/mfu": 0.007734825762862299, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 37, "data": {"train/loss": 0.6320319175720215, "train/grad_norm": 5.728127956390381, "train/lr": 1.9000000000000002e-06, "train/mfu": 0.0076635483305446915, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 38, "data": {"train/loss": 0.6486983299255371, "train/grad_norm": 5.706100940704346, "train/lr": 1.9500000000000004e-06, "train/mfu": 0.007675017972593516, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
}
{"step": 39, "data": {"train/loss": 0.6419638395309448, "train/grad_norm": 6.1897454261779785, "train/lr": 1.9500000000000004e-06, "train/mfu": 0.007742252596868556, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 40, "data": {"train/loss": 0.6119746565818787, "train/grad_norm": 5.412203788757324, "train/lr": 2.0000000000000003e-06, "train/mfu": 0.007642482720231917, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 41, "data": {"train/loss": 0.5906100273132324, "train/grad_norm": 4.806284427642822, "train/lr": 2.05e-06, "train/mfu": 0.007683610967330996, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 42, "data": {"train/loss": 0.5555201768875122, "train/grad_norm": 3.581298351287842, "train/lr": 2.1000000000000002e-06, "train/mfu": 0.007730881731874173, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 43, "data": {"train/loss": 0.5742901563644409, "train/grad_norm": 3.0578088760375977, "train/lr": 2.15e-06, "train/mfu": 0.007756821505880147, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 44, "data": {"train/loss": 0.6464570760726929, "train/grad_norm": 2.9978229999542236, "train/lr": 2.2e-06, "train/mfu": 0.007858948935138397, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 45, "data": {"train/loss": 0.5959900617599487, "train/grad_norm": 2.8930859565734863, "train/lr": 2.25e-06, "train/mfu": 0.007745372456051956, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 46, "data": {"train/loss": 0.5689729452133179, "train/grad_norm": 2.61911940574646, "train/lr": 2.3000000000000004e-06, "train/mfu": 0.007724528275287103, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 47, "data": {"train/loss": 0.5197640657424927, "train/grad_norm": 2.2883613109588623, "train/lr": 2.35e-06, "train/mfu": 0.007727747067356922, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 48, "data": {"train/loss": 0.564143180847168, "train/grad_norm": 2.3040964603424072, "train/lr": 2.4000000000000003e-06, "train/mfu": 0.007707716837452561, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 49, "data": {"train/loss": 0.5122305750846863, "train/grad_norm": 2.4179606437683105, "train/lr": 2.4500000000000003e-06, "train/mfu": 0.007801685360121557, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 50, "data": {"train/loss": 0.5995305776596069, "train/grad_norm": 2.2803187370300293, "train/lr": 2.5e-06, "train/mfu": 0.007794776648629456, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 51, "data": {"train/loss": 0.5333773493766785, "train/grad_norm": 2.11323618888855, "train/lr": 2.55e-06, "train/mfu": 0.009800540338028343, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 52, "data": {"train/loss": 0.5176950693130493, "train/grad_norm": 2.4126195907592773, "train/lr": 2.6e-06, "train/mfu": 0.010110280215992513, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 53, "data": {"train/loss": 0.5462563037872314, "train/grad_norm": 2.132896900177002, "train/lr": 2.6500000000000005e-06, "train/mfu": 0.008653193941367038, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 54, "data": {"train/loss": 0.5350499153137207, "train/grad_norm": 2.1988844871520996, "train/lr": 2.7000000000000004e-06, "train/mfu": 0.007947505838854299, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 55, "data": {"train/loss": 0.5270505547523499, "train/grad_norm": 2.089387893676758, "train/lr": 2.7500000000000004e-06, "train/mfu": 0.00771583131426248, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 56, "data": {"train/loss": 0.5217511653900146, "train/grad_norm": 2.0679397583007812, "train/lr": 2.8000000000000003e-06, "train/mfu": 0.007721556499747921, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 57, "data": {"train/loss": 0.48338112235069275, "train/grad_norm": 2.0592048168182373, "train/lr": 2.85e-06, "train/mfu": 0.00771194431830916, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 58, "data": {"train/loss": 0.5306092500686646, "train/grad_norm": 1.7933896780014038, "train/lr": 2.9e-06, "train/mfu": 0.007696234783090353, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 58, "data": {"val/loss": 0.5354019999504089}}
{"step": 59, "data": {"train/loss": 0.5073105692863464, "train/grad_norm": 1.9348957538604736, "train/lr": 2.95e-06, "train/mfu": 0.0077606220735835845, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 60, "data": {"train/loss": 0.4983723759651184, "train/grad_norm": 1.7383158206939697, "train/lr": 3e-06, "train/mfu": 0.007806974381769906, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 61, "data": {"train/loss": 0.4955832362174988, "train/grad_norm": 1.643126368522644, "train/lr": 3.05e-06, "train/mfu": 0.0077500736096279416, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 62, "data": {"train/loss": 0.4821028709411621, "train/grad_norm": 1.7776232957839966, "train/lr": 3.1000000000000004e-06, "train/mfu": 0.007706193002245168, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 63, "data": {"train/loss": 0.5125930905342102, "train/grad_norm": 1.7083508968353271, "train/lr": 3.1500000000000003e-06, "train/mfu": 0.007795438866840644, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 64, "data": {"train/loss": 0.489887535572052, "train/grad_norm": 1.6380802392959595, "train/lr": 3.2000000000000003e-06, "train/mfu": 0.007754580060683099, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 65, "data": {"train/loss": 0.46068406105041504, "train/grad_norm": 1.7726702690124512, "train/lr": 3.2500000000000002e-06, "train/mfu": 0.007750807361540122, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 66, "data": {"train/loss": 0.47721925377845764, "train/grad_norm": 1.5348423719406128, "train/lr": 3.3000000000000006e-06, "train/mfu": 0.0076773251328826845, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 67, "data": {"train/loss": 0.463970422744751, "train/grad_norm": 1.548851728439331, "train/lr": 3.3500000000000005e-06, "train/mfu": 0.007687050452920443, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 68, "data": {"train/loss": 0.49680858850479126, "train/grad_norm": 1.6723589897155762, "train/lr": 3.4000000000000005e-06, "train/mfu": 0.0077929091854110275, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 69, "data": {"train/loss": 0.4888628423213959, "train/grad_norm": 1.540182113647461, "train/lr": 3.45e-06, "train/mfu": 0.007761598865380451, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 70, "data": {"train/loss": 0.45106545090675354, "train/grad_norm": 1.5375646352767944, "train/lr": 3.5e-06, "train/mfu": 0.007765299932788394, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 71, "data": {"train/loss": 0.47185802459716797, "train/grad_norm": 1.639747142791748, "train/lr": 3.5500000000000003e-06, "train/mfu": 0.007722788933608523, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 72, "data": {"train/loss": 0.4816240966320038, "train/grad_norm": 1.6553130149841309, "train/lr": 3.6000000000000003e-06, "train/mfu": 0.0077210059203844175, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 73, "data": {"train/loss": 0.4628792405128479, "train/grad_norm": 1.691836953163147, "train/lr": 3.65e-06, "train/mfu": 0.008109065608302815, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 74, "data": {"train/loss": 0.4557870030403137, "train/grad_norm": 1.5554072856903076, "train/lr": 3.7e-06, "train/mfu": 0.007951848892631654, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 75, "data": {"train/loss": 0.46370595693588257, "train/grad_norm": 1.580449104309082, "train/lr": 3.7500000000000005e-06, "train/mfu": 0.007971394058994314, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 76, "data": {"train/loss": 0.49920666217803955, "train/grad_norm": 1.6906574964523315, "train/lr": 3.8000000000000005e-06, "train/mfu": 0.008053547114828622, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
