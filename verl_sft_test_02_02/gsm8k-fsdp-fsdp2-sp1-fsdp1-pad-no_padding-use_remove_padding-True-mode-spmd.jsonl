{"step": 1, "data": {"grad_norm": 22.61558723449707, "train/loss": 1.371539831161499, "train/lr": 5.0000000000000004e-08, "train/mfu": 0.010046364047111815, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 2, "data": {"grad_norm": 22.199386596679688, "train/loss": 1.4289494752883911, "train/lr": 1.0000000000000001e-07, "train/mfu": 0.014195366810920517, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 3, "data": {"grad_norm": 21.36765480041504, "train/loss": 1.3436613082885742, "train/lr": 1.5000000000000002e-07, "train/mfu": 0.01585121782068992, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 4, "data": {"grad_norm": 21.60664176940918, "train/loss": 1.3845096826553345, "train/lr": 2.0000000000000002e-07, "train/mfu": 0.015857280611681665, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 5, "data": {"grad_norm": 22.879257202148438, "train/loss": 1.4229539632797241, "train/lr": 2.5000000000000004e-07, "train/mfu": 0.01588522251518893, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 6, "data": {"grad_norm": 20.754179000854492, "train/loss": 1.3689957857131958, "train/lr": 3.0000000000000004e-07, "train/mfu": 0.016013580162615726, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 7, "data": {"grad_norm": 22.43135643005371, "train/loss": 1.3783915042877197, "train/lr": 3.5000000000000004e-07, "train/mfu": 0.015701455464166434, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 8, "data": {"grad_norm": 22.70256233215332, "train/loss": 1.3690714836120605, "train/lr": 4.0000000000000003e-07, "train/mfu": 0.020165735918541136, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 9, "data": {"grad_norm": 22.213457107543945, "train/loss": 1.3349703550338745, "train/lr": 4.5000000000000003e-07, "train/mfu": 0.023225564234876546, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 10, "data": {"grad_norm": 23.184053421020508, "train/loss": 1.4386394023895264, "train/lr": 5.000000000000001e-07, "train/mfu": 0.015574737377859292, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 11, "data": {"grad_norm": 20.963329315185547, "train/loss": 1.3923053741455078, "train/lr": 5.5e-07, "train/mfu": 0.015839543819244673, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 12, "data": {"grad_norm": 21.39254379272461, "train/loss": 1.3786752223968506, "train/lr": 6.000000000000001e-07, "train/mfu": 0.023604616174496496, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 13, "data": {"grad_norm": 20.87495231628418, "train/loss": 1.3611695766448975, "train/lr": 6.5e-07, "train/mfu": 0.000678048626489372, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 14, "data": {"grad_norm": 19.462711334228516, "train/loss": 1.352583885192871, "train/lr": 7.000000000000001e-07, "train/mfu": 0.015846653845024254, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 15, "data": {"grad_norm": 19.520994186401367, "train/loss": 1.3268965482711792, "train/lr": 7.5e-07, "train/mfu": 0.016204839293154902, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 16, "data": {"grad_norm": 17.303237915039062, "train/loss": 1.2699917554855347, "train/lr": 8.000000000000001e-07, "train/mfu": 0.01625542675844134, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 17, "data": {"grad_norm": 18.354633331298828, "train/loss": 1.342411756515503, "train/lr": 8.500000000000001e-07, "train/mfu": 0.016578271729687687, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 18, "data": {"grad_norm": 17.341228485107422, "train/loss": 1.2850589752197266, "train/lr": 9.000000000000001e-07, "train/mfu": 0.015916899006750862, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 19, "data": {"grad_norm": 19.017282485961914, "train/loss": 1.2865357398986816, "train/lr": 9.500000000000001e-07, "train/mfu": 0.016337391160696855, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 20, "data": {"grad_norm": 16.255041122436523, "train/loss": 1.1783337593078613, "train/lr": 1.0000000000000002e-06, "train/mfu": 0.016600214579355614, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 21, "data": {"grad_norm": 16.42186737060547, "train/loss": 1.1858340501785278, "train/lr": 1.0500000000000001e-06, "train/mfu": 0.02370673870530487, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 22, "data": {"grad_norm": 15.79654598236084, "train/loss": 1.1506855487823486, "train/lr": 1.1e-06, "train/mfu": 0.0024967156213520927, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 23, "data": {"grad_norm": 16.254375457763672, "train/loss": 1.1236610412597656, "train/lr": 1.1500000000000002e-06, "train/mfu": 0.02088492261118141, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 24, "data": {"grad_norm": 16.515607833862305, "train/loss": 1.1235296726226807, "train/lr": 1.2000000000000002e-06, "train/mfu": 0.002450900635778996, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 25, "data": {"grad_norm": 19.030996322631836, "train/loss": 1.1642110347747803, "train/lr": 1.25e-06, "train/mfu": 0.02083529124828538, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 26, "data": {"grad_norm": 17.750139236450195, "train/loss": 1.0580862760543823, "train/lr": 1.3e-06, "train/mfu": 0.0024325672871270544, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 27, "data": {"grad_norm": 10.775315284729004, "train/loss": 0.9027862548828125, "train/lr": 1.3500000000000002e-06, "train/mfu": 0.02015051218574107, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 28, "data": {"grad_norm": 10.137136459350586, "train/loss": 0.8743250370025635, "train/lr": 1.4000000000000001e-06, "train/mfu": 0.0024969756954558294, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 29, "data": {"grad_norm": 9.865825653076172, "train/loss": 0.8671129941940308, "train/lr": 1.45e-06, "train/mfu": 0.018434503634736255, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 30, "data": {"grad_norm": 10.109439849853516, "train/loss": 0.8451166152954102, "train/lr": 1.5e-06, "train/mfu": 0.0023409351732283533, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 31, "data": {"grad_norm": 9.280804634094238, "train/loss": 0.7956030368804932, "train/lr": 1.5500000000000002e-06, "train/mfu": 0.017921399125265112, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 32, "data": {"grad_norm": 8.925850868225098, "train/loss": 0.8300800919532776, "train/lr": 1.6000000000000001e-06, "train/mfu": 0.02341389639468423, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 33, "data": {"grad_norm": 8.738760948181152, "train/loss": 0.8441966772079468, "train/lr": 1.6500000000000003e-06, "train/mfu": 0.0022536959002544513, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 34, "data": {"grad_norm": 8.475561141967773, "train/loss": 0.7498762607574463, "train/lr": 1.7000000000000002e-06, "train/mfu": 0.023236750789380943, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 35, "data": {"grad_norm": 7.471158027648926, "train/loss": 0.7242812514305115, "train/lr": 1.75e-06, "train/mfu": 0.0023142149884582243, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 36, "data": {"grad_norm": 6.2965087890625, "train/loss": 0.7254933714866638, "train/lr": 1.8000000000000001e-06, "train/mfu": 0.023754338749573077, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 37, "data": {"grad_norm": 5.730563163757324, "train/loss": 0.660862922668457, "train/lr": 1.85e-06, "train/mfu": 0.002276029284934189, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 38, "data": {"grad_norm": 5.820574760437012, "train/loss": 0.6684252619743347, "train/lr": 1.9000000000000002e-06, "train/mfu": 0.022063670540028217, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 39, "data": {"grad_norm": 6.195858001708984, "train/loss": 0.6410384178161621, "train/lr": 1.9500000000000004e-06, "train/mfu": 0.0023758752208526326, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 40, "data": {"grad_norm": 5.323445796966553, "train/loss": 0.6106613874435425, "train/lr": 2.0000000000000003e-06, "train/mfu": 0.02233711216704876, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 41, "data": {"grad_norm": 4.6959733963012695, "train/loss": 0.5900793075561523, "train/lr": 2.05e-06, "train/mfu": 0.0024243568471395962, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 42, "data": {"grad_norm": 3.5242486000061035, "train/loss": 0.5548199415206909, "train/lr": 2.1000000000000002e-06, "train/mfu": 0.020271009536923616, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 43, "data": {"grad_norm": 2.993062973022461, "train/loss": 0.5736421942710876, "train/lr": 2.15e-06, "train/mfu": 0.002358202801943455, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 44, "data": {"grad_norm": 2.9741392135620117, "train/loss": 0.6453601121902466, "train/lr": 2.2e-06, "train/mfu": 0.020221660687882687, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 45, "data": {"grad_norm": 2.827254295349121, "train/loss": 0.5948672294616699, "train/lr": 2.25e-06, "train/mfu": 0.0024954791382021755, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 46, "data": {"grad_norm": 2.563586473464966, "train/loss": 0.5683068633079529, "train/lr": 2.3000000000000004e-06, "train/mfu": 0.01929099594596219, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 47, "data": {"grad_norm": 2.2838971614837646, "train/loss": 0.5188943147659302, "train/lr": 2.35e-06, "train/mfu": 0.0024653760234122496, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 48, "data": {"grad_norm": 2.2901504039764404, "train/loss": 0.5634527206420898, "train/lr": 2.4000000000000003e-06, "train/mfu": 0.0194235282526901, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 49, "data": {"grad_norm": 2.3930017948150635, "train/loss": 0.5119607448577881, "train/lr": 2.4500000000000003e-06, "train/mfu": 0.0022824347872859403, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 50, "data": {"grad_norm": 2.267824172973633, "train/loss": 0.5989794135093689, "train/lr": 2.5e-06, "train/mfu": 0.02048520976268386, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 51, "data": {"grad_norm": 2.1133880615234375, "train/loss": 0.5330881476402283, "train/lr": 2.55e-06, "train/mfu": 0.02389124930700488, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 52, "data": {"grad_norm": 2.3791685104370117, "train/loss": 0.5174950957298279, "train/lr": 2.6e-06, "train/mfu": 0.0023857225378638713, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 53, "data": {"grad_norm": 2.1471593379974365, "train/loss": 0.5459293127059937, "train/lr": 2.6500000000000005e-06, "train/mfu": 0.023805864780210385, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 54, "data": {"grad_norm": 2.1852831840515137, "train/loss": 0.5344952344894409, "train/lr": 2.7000000000000004e-06, "train/mfu": 0.0022690387383768845, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 55, "data": {"grad_norm": 2.0740582942962646, "train/loss": 0.5267825126647949, "train/lr": 2.7500000000000004e-06, "train/mfu": 0.024033838948411896, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 56, "data": {"grad_norm": 2.0628769397735596, "train/loss": 0.5212815999984741, "train/lr": 2.8000000000000003e-06, "train/mfu": 0.002365697259609436, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 57, "data": {"grad_norm": 2.011791706085205, "train/loss": 0.48312243819236755, "train/lr": 2.85e-06, "train/mfu": 0.023618326180031988, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 58, "data": {"grad_norm": 1.8214110136032104, "train/loss": 0.5300697684288025, "train/lr": 2.9e-06, "train/mfu": 0.00243816867195747, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 58, "data": {"val/loss": 0.5350148677825928}}
{"step": 59, "data": {"grad_norm": 1.9330445528030396, "train/loss": 0.5069936513900757, "train/lr": 2.95e-06, "train/mfu": 0.023213729620185898, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 60, "data": {"grad_norm": 1.732919692993164, "train/loss": 0.49852752685546875, "train/lr": 3e-06, "train/mfu": 0.0022990461642234633, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 61, "data": {"grad_norm": 1.6551198959350586, "train/loss": 0.49565985798835754, "train/lr": 3.05e-06, "train/mfu": 0.023905866272430316, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 62, "data": {"grad_norm": 1.7477853298187256, "train/loss": 0.4816091060638428, "train/lr": 3.1000000000000004e-06, "train/mfu": 0.0023225815604598155, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 63, "data": {"grad_norm": 1.6820820569992065, "train/loss": 0.5113396048545837, "train/lr": 3.1500000000000003e-06, "train/mfu": 0.02320197404418846, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 64, "data": {"grad_norm": 1.8139435052871704, "train/loss": 0.48979824781417847, "train/lr": 3.2000000000000003e-06, "train/mfu": 0.002354213681892576, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 65, "data": {"grad_norm": 1.709074854850769, "train/loss": 0.4603734016418457, "train/lr": 3.2500000000000002e-06, "train/mfu": 0.023794623194445197, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 66, "data": {"grad_norm": 1.5340242385864258, "train/loss": 0.4770795702934265, "train/lr": 3.3000000000000006e-06, "train/mfu": 0.002380792095365315, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 67, "data": {"grad_norm": 1.5439895391464233, "train/loss": 0.46315157413482666, "train/lr": 3.3500000000000005e-06, "train/mfu": 0.023205214290423017, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 68, "data": {"grad_norm": 1.6639257669448853, "train/loss": 0.4966811537742615, "train/lr": 3.4000000000000005e-06, "train/mfu": 0.0022951522947352266, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 69, "data": {"grad_norm": 1.5707199573516846, "train/loss": 0.4886782467365265, "train/lr": 3.45e-06, "train/mfu": 0.023489137504964175, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 70, "data": {"grad_norm": 1.5267765522003174, "train/loss": 0.45106640458106995, "train/lr": 3.5e-06, "train/mfu": 0.0023665100678998767, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 71, "data": {"grad_norm": 1.6143559217453003, "train/loss": 0.4714702367782593, "train/lr": 3.5500000000000003e-06, "train/mfu": 0.023553109516938106, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 72, "data": {"grad_norm": 1.631313443183899, "train/loss": 0.48120227456092834, "train/lr": 3.6000000000000003e-06, "train/mfu": 0.0022377968708243564, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 73, "data": {"grad_norm": 1.6988799571990967, "train/loss": 0.4625808596611023, "train/lr": 3.65e-06, "train/mfu": 0.023190032018071863, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 74, "data": {"grad_norm": 1.5636990070343018, "train/loss": 0.45554736256599426, "train/lr": 3.7e-06, "train/mfu": 0.0023185858221186776, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 75, "data": {"grad_norm": 1.570127248764038, "train/loss": 0.46346479654312134, "train/lr": 3.7500000000000005e-06, "train/mfu": 0.023753782333225636, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 76, "data": {"grad_norm": 1.6429855823516846, "train/loss": 0.4991157650947571, "train/lr": 3.8000000000000005e-06, "train/mfu": 0.002408915571970119, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 77, "data": {"grad_norm": 1.4483410120010376, "train/loss": 0.4307019114494324, "train/lr": 3.85e-06, "train/mfu": 0.023489054263102556, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 78, "data": {"grad_norm": 1.4910684823989868, "train/loss": 0.44695067405700684, "train/lr": 3.900000000000001e-06, "train/mfu": 0.0023167367870906464, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 79, "data": {"grad_norm": 1.6008288860321045, "train/loss": 0.47633230686187744, "train/lr": 3.95e-06, "train/mfu": 0.023769396106961407, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 80, "data": {"grad_norm": 1.5017343759536743, "train/loss": 0.47080856561660767, "train/lr": 4.000000000000001e-06, "train/mfu": 0.023546912919888877, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 81, "data": {"grad_norm": 1.5335842370986938, "train/loss": 0.49553149938583374, "train/lr": 4.05e-06, "train/mfu": 0.0023371175742595268, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 82, "data": {"grad_norm": 1.5442942380905151, "train/loss": 0.4676103889942169, "train/lr": 4.1e-06, "train/mfu": 0.02323351652784638, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 83, "data": {"grad_norm": 1.4758234024047852, "train/loss": 0.44699716567993164, "train/lr": 4.15e-06, "train/mfu": 0.002395131541760649, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 84, "data": {"grad_norm": 1.457258701324463, "train/loss": 0.45125067234039307, "train/lr": 4.2000000000000004e-06, "train/mfu": 0.02378573674831311, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 85, "data": {"grad_norm": 1.788256287574768, "train/loss": 0.49039027094841003, "train/lr": 4.25e-06, "train/mfu": 0.002268086832596825, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 86, "data": {"grad_norm": 1.6085716485977173, "train/loss": 0.46346789598464966, "train/lr": 4.3e-06, "train/mfu": 0.023508059698460416, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 87, "data": {"grad_norm": 1.5810211896896362, "train/loss": 0.4506155252456665, "train/lr": 4.350000000000001e-06, "train/mfu": 0.002277713104340433, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 88, "data": {"grad_norm": 1.547082543373108, "train/loss": 0.4673222303390503, "train/lr": 4.4e-06, "train/mfu": 0.023838293187281524, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 89, "data": {"grad_norm": 1.5728133916854858, "train/loss": 0.4795842468738556, "train/lr": 4.450000000000001e-06, "train/mfu": 0.0022487035533030244, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 90, "data": {"grad_norm": 1.6426060199737549, "train/loss": 0.4713571071624756, "train/lr": 4.5e-06, "train/mfu": 0.023394510631968134, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 91, "data": {"grad_norm": 1.5244392156600952, "train/loss": 0.44149449467658997, "train/lr": 4.5500000000000005e-06, "train/mfu": 0.002312972087042116, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 92, "data": {"grad_norm": 1.5461729764938354, "train/loss": 0.46915584802627563, "train/lr": 4.600000000000001e-06, "train/mfu": 0.023727626912837394, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 93, "data": {"grad_norm": 1.4840846061706543, "train/loss": 0.47253119945526123, "train/lr": 4.65e-06, "train/mfu": 0.002203891289144725, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 94, "data": {"grad_norm": 1.4311938285827637, "train/loss": 0.4565820097923279, "train/lr": 4.7e-06, "train/mfu": 0.02335449620148636, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 95, "data": {"grad_norm": 1.5666334629058838, "train/loss": 0.4707557260990143, "train/lr": 4.75e-06, "train/mfu": 0.0024353512061138015, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 96, "data": {"grad_norm": 1.4522888660430908, "train/loss": 0.4720204472541809, "train/lr": 4.800000000000001e-06, "train/mfu": 0.023534849941116303, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 97, "data": {"grad_norm": 1.5609726905822754, "train/loss": 0.4736366271972656, "train/lr": 4.85e-06, "train/mfu": 0.002369683331323702, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 98, "data": {"grad_norm": 1.4848397970199585, "train/loss": 0.4233188033103943, "train/lr": 4.9000000000000005e-06, "train/mfu": 0.02342191177885942, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 99, "data": {"grad_norm": 1.4958069324493408, "train/loss": 0.5000937581062317, "train/lr": 4.95e-06, "train/mfu": 0.0023076195764843684, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 100, "data": {"grad_norm": 1.4787931442260742, "train/loss": 0.450397789478302, "train/lr": 5e-06, "train/mfu": 0.023447272561921667, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 101, "data": {"grad_norm": 1.4089605808258057, "train/loss": 0.4250381588935852, "train/lr": 5.050000000000001e-06, "train/mfu": 0.0024002929587021394, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 102, "data": {"grad_norm": 1.6574686765670776, "train/loss": 0.5041890740394592, "train/lr": 5.1e-06, "train/mfu": 0.023665542153336223, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 103, "data": {"grad_norm": 1.5098464488983154, "train/loss": 0.4493390917778015, "train/lr": 5.150000000000001e-06, "train/mfu": 0.002247547550100916, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 104, "data": {"grad_norm": 1.4281654357910156, "train/loss": 0.4524083733558655, "train/lr": 5.2e-06, "train/mfu": 0.023750007708527695, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 105, "data": {"grad_norm": 1.4564207792282104, "train/loss": 0.430355966091156, "train/lr": 5.2500000000000006e-06, "train/mfu": 0.002422643192972933, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 106, "data": {"grad_norm": 1.4804675579071045, "train/loss": 0.439666211605072, "train/lr": 5.300000000000001e-06, "train/mfu": 0.023547397683030224, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 107, "data": {"grad_norm": 1.4828001260757446, "train/loss": 0.447889119386673, "train/lr": 5.3500000000000004e-06, "train/mfu": 0.0023584647571120083, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 108, "data": {"grad_norm": 1.514147400856018, "train/loss": 0.46328556537628174, "train/lr": 5.400000000000001e-06, "train/mfu": 0.0235162608691721, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 109, "data": {"grad_norm": 1.4188612699508667, "train/loss": 0.43002456426620483, "train/lr": 5.450000000000001e-06, "train/mfu": 0.023491139386876934, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 110, "data": {"grad_norm": 1.5075280666351318, "train/loss": 0.4564681053161621, "train/lr": 5.500000000000001e-06, "train/mfu": 0.002373569266220376, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 111, "data": {"grad_norm": 1.512421727180481, "train/loss": 0.43800073862075806, "train/lr": 5.550000000000001e-06, "train/mfu": 0.02373229847497005, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 112, "data": {"grad_norm": 1.4625701904296875, "train/loss": 0.4421801269054413, "train/lr": 5.600000000000001e-06, "train/mfu": 0.0023213134328579996, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 113, "data": {"grad_norm": 1.303665041923523, "train/loss": 0.4160144329071045, "train/lr": 5.65e-06, "train/mfu": 0.023546511041581394, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 114, "data": {"grad_norm": 1.4926159381866455, "train/loss": 0.4758063554763794, "train/lr": 5.7e-06, "train/mfu": 0.002355036366973111, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 115, "data": {"grad_norm": 1.5640372037887573, "train/loss": 0.4751589596271515, "train/lr": 5.75e-06, "train/mfu": 0.024077530644554243, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 116, "data": {"grad_norm": 1.4925475120544434, "train/loss": 0.48466336727142334, "train/lr": 5.8e-06, "train/mfu": 0.0023629957644775083, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 116, "data": {"val/loss": 0.4677685797214508}}
