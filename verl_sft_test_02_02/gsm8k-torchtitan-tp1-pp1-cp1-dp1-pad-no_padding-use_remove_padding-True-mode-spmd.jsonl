{"step": 1, "data": {"grad_norm": 22.656513214111328, "train/loss": 1.3719072341918945, "train/lr": 5.0000000000000004e-08, "train/mfu": 0.016082925606228066, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 2, "data": {"grad_norm": 22.2428035736084, "train/loss": 1.4301011562347412, "train/lr": 1.0000000000000001e-07, "train/mfu": 0.021712066458816317, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 3, "data": {"grad_norm": 21.375900268554688, "train/loss": 1.3443446159362793, "train/lr": 1.5000000000000002e-07, "train/mfu": 0.0257998936823614, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 4, "data": {"grad_norm": 21.58414077758789, "train/loss": 1.3850607872009277, "train/lr": 2.0000000000000002e-07, "train/mfu": 0.025325287284700564, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 5, "data": {"grad_norm": 22.85614013671875, "train/loss": 1.4224333763122559, "train/lr": 2.5000000000000004e-07, "train/mfu": 0.02557253966353781, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 6, "data": {"grad_norm": 20.768131256103516, "train/loss": 1.3696463108062744, "train/lr": 3.0000000000000004e-07, "train/mfu": 0.025783924685851174, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 7, "data": {"grad_norm": 22.357131958007812, "train/loss": 1.3784507513046265, "train/lr": 3.5000000000000004e-07, "train/mfu": 0.025908505532564397, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 8, "data": {"grad_norm": 22.640697479248047, "train/loss": 1.3685485124588013, "train/lr": 4.0000000000000003e-07, "train/mfu": 0.02583991334919033, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 9, "data": {"grad_norm": 22.201704025268555, "train/loss": 1.3350045680999756, "train/lr": 4.5000000000000003e-07, "train/mfu": 0.025434115139582162, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 10, "data": {"grad_norm": 23.207788467407227, "train/loss": 1.43887197971344, "train/lr": 5.000000000000001e-07, "train/mfu": 0.025417712092876688, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 11, "data": {"grad_norm": 21.00111961364746, "train/loss": 1.3931776285171509, "train/lr": 5.5e-07, "train/mfu": 0.02543157501064045, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 12, "data": {"grad_norm": 21.360698699951172, "train/loss": 1.3780732154846191, "train/lr": 6.000000000000001e-07, "train/mfu": 0.02582001722405193, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 13, "data": {"grad_norm": 20.782686233520508, "train/loss": 1.3602921962738037, "train/lr": 6.5e-07, "train/mfu": 0.026004425348186044, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 14, "data": {"grad_norm": 19.461915969848633, "train/loss": 1.3527004718780518, "train/lr": 7.000000000000001e-07, "train/mfu": 0.02604885142253654, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 15, "data": {"grad_norm": 19.509422302246094, "train/loss": 1.3257291316986084, "train/lr": 7.5e-07, "train/mfu": 0.02577000526691736, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 16, "data": {"grad_norm": 17.27943229675293, "train/loss": 1.2692675590515137, "train/lr": 8.000000000000001e-07, "train/mfu": 0.025843374026613228, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 17, "data": {"grad_norm": 18.342323303222656, "train/loss": 1.3423041105270386, "train/lr": 8.500000000000001e-07, "train/mfu": 0.02619664094570403, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 18, "data": {"grad_norm": 17.303985595703125, "train/loss": 1.2835599184036255, "train/lr": 9.000000000000001e-07, "train/mfu": 0.025781622502844295, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 19, "data": {"grad_norm": 18.986679077148438, "train/loss": 1.2850513458251953, "train/lr": 9.500000000000001e-07, "train/mfu": 0.0257793266255511, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 20, "data": {"grad_norm": 16.28532600402832, "train/loss": 1.1770555973052979, "train/lr": 1.0000000000000002e-06, "train/mfu": 0.02640783584235447, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 21, "data": {"grad_norm": 16.433237075805664, "train/loss": 1.1847227811813354, "train/lr": 1.0500000000000001e-06, "train/mfu": 0.02628854426633995, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 22, "data": {"grad_norm": 15.83185863494873, "train/loss": 1.1494497060775757, "train/lr": 1.1e-06, "train/mfu": 0.02569762860051662, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 23, "data": {"grad_norm": 16.278478622436523, "train/loss": 1.1220521926879883, "train/lr": 1.1500000000000002e-06, "train/mfu": 0.025857324360722342, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 24, "data": {"grad_norm": 16.55176544189453, "train/loss": 1.1223305463790894, "train/lr": 1.2000000000000002e-06, "train/mfu": 0.025798916304343958, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 25, "data": {"grad_norm": 19.12995147705078, "train/loss": 1.1613572835922241, "train/lr": 1.25e-06, "train/mfu": 0.025546650015542596, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 26, "data": {"grad_norm": 17.773170471191406, "train/loss": 1.0555734634399414, "train/lr": 1.3e-06, "train/mfu": 0.02584969741941998, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 27, "data": {"grad_norm": 10.749567985534668, "train/loss": 0.9020155072212219, "train/lr": 1.3500000000000002e-06, "train/mfu": 0.026181395375563687, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 28, "data": {"grad_norm": 10.140152931213379, "train/loss": 0.874190092086792, "train/lr": 1.4000000000000001e-06, "train/mfu": 0.025898209746108045, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 29, "data": {"grad_norm": 9.843786239624023, "train/loss": 0.8663039207458496, "train/lr": 1.45e-06, "train/mfu": 0.02559194289160837, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 30, "data": {"grad_norm": 10.105484962463379, "train/loss": 0.8439148664474487, "train/lr": 1.5e-06, "train/mfu": 0.025952774981434407, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 31, "data": {"grad_norm": 9.296887397766113, "train/loss": 0.7949183583259583, "train/lr": 1.5500000000000002e-06, "train/mfu": 0.026358171622089272, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 32, "data": {"grad_norm": 8.901098251342773, "train/loss": 0.8289040327072144, "train/lr": 1.6000000000000001e-06, "train/mfu": 0.02577114435992609, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 33, "data": {"grad_norm": 8.72154426574707, "train/loss": 0.842971920967102, "train/lr": 1.6500000000000003e-06, "train/mfu": 0.0258275364528385, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 34, "data": {"grad_norm": 8.42931842803955, "train/loss": 0.7483645677566528, "train/lr": 1.7000000000000002e-06, "train/mfu": 0.025635684756113, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 35, "data": {"grad_norm": 7.434720039367676, "train/loss": 0.7231370806694031, "train/lr": 1.75e-06, "train/mfu": 0.026010064331644406, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 36, "data": {"grad_norm": 6.265286922454834, "train/loss": 0.7253224849700928, "train/lr": 1.8000000000000001e-06, "train/mfu": 0.02599650130812852, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 37, "data": {"grad_norm": 5.690404891967773, "train/loss": 0.660273551940918, "train/lr": 1.85e-06, "train/mfu": 0.025623240761770767, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 38, "data": {"grad_norm": 5.863643169403076, "train/loss": 0.6683844327926636, "train/lr": 1.9000000000000002e-06, "train/mfu": 0.025765536670994425, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 39, "data": {"grad_norm": 6.133294105529785, "train/loss": 0.6412338018417358, "train/lr": 1.9500000000000004e-06, "train/mfu": 0.025629836276850667, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 40, "data": {"grad_norm": 5.296927452087402, "train/loss": 0.6108369827270508, "train/lr": 2.0000000000000003e-06, "train/mfu": 0.025748047581062902, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 41, "data": {"grad_norm": 4.773255825042725, "train/loss": 0.5906750559806824, "train/lr": 2.05e-06, "train/mfu": 0.025769681805171484, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 42, "data": {"grad_norm": 3.5160372257232666, "train/loss": 0.5546262264251709, "train/lr": 2.1000000000000002e-06, "train/mfu": 0.02580375063677765, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 43, "data": {"grad_norm": 3.0123844146728516, "train/loss": 0.5732150673866272, "train/lr": 2.15e-06, "train/mfu": 0.02624497388196872, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 44, "data": {"grad_norm": 2.980696201324463, "train/loss": 0.6453030109405518, "train/lr": 2.2e-06, "train/mfu": 0.02610327275636879, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 45, "data": {"grad_norm": 3.0160391330718994, "train/loss": 0.594810962677002, "train/lr": 2.25e-06, "train/mfu": 0.025937881854710246, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 46, "data": {"grad_norm": 2.576752185821533, "train/loss": 0.5681195259094238, "train/lr": 2.3000000000000004e-06, "train/mfu": 0.025265267283889104, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 47, "data": {"grad_norm": 2.286895751953125, "train/loss": 0.5190522074699402, "train/lr": 2.35e-06, "train/mfu": 0.025699007038364988, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 48, "data": {"grad_norm": 2.3061604499816895, "train/loss": 0.5634639859199524, "train/lr": 2.4000000000000003e-06, "train/mfu": 0.025795585673903592, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 49, "data": {"grad_norm": 2.4174721240997314, "train/loss": 0.511457085609436, "train/lr": 2.4500000000000003e-06, "train/mfu": 0.025721337166743224, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 50, "data": {"grad_norm": 2.2650842666625977, "train/loss": 0.59908127784729, "train/lr": 2.5e-06, "train/mfu": 0.02592148053627536, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 51, "data": {"grad_norm": 2.0954244136810303, "train/loss": 0.5328871607780457, "train/lr": 2.55e-06, "train/mfu": 0.026078473900874536, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 52, "data": {"grad_norm": 2.410201072692871, "train/loss": 0.5171859860420227, "train/lr": 2.6e-06, "train/mfu": 0.025620493674451447, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 53, "data": {"grad_norm": 2.133249044418335, "train/loss": 0.5457010269165039, "train/lr": 2.6500000000000005e-06, "train/mfu": 0.025674550956557393, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 54, "data": {"grad_norm": 2.2144222259521484, "train/loss": 0.5347293615341187, "train/lr": 2.7000000000000004e-06, "train/mfu": 0.02593098595738557, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 55, "data": {"grad_norm": 2.078887701034546, "train/loss": 0.5266538858413696, "train/lr": 2.7500000000000004e-06, "train/mfu": 0.0260654052780053, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 56, "data": {"grad_norm": 2.0714313983917236, "train/loss": 0.5216790437698364, "train/lr": 2.8000000000000003e-06, "train/mfu": 0.02562718557518256, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 57, "data": {"grad_norm": 2.0306708812713623, "train/loss": 0.4826458692550659, "train/lr": 2.85e-06, "train/mfu": 0.025714930684245154, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 58, "data": {"grad_norm": 1.7834659814834595, "train/loss": 0.5303258895874023, "train/lr": 2.9e-06, "train/mfu": 0.026093012184984544, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 58, "data": {"val/loss": 0.5348981022834778}}
{"step": 59, "data": {"grad_norm": 1.9126617908477783, "train/loss": 0.5067239999771118, "train/lr": 2.95e-06, "train/mfu": 0.025527958205588494, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 60, "data": {"grad_norm": 1.7244224548339844, "train/loss": 0.4980260133743286, "train/lr": 3e-06, "train/mfu": 0.025479461366690066, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 61, "data": {"grad_norm": 1.6565215587615967, "train/loss": 0.49535712599754333, "train/lr": 3.05e-06, "train/mfu": 0.026247751177629623, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 62, "data": {"grad_norm": 1.7713905572891235, "train/loss": 0.4816371500492096, "train/lr": 3.1000000000000004e-06, "train/mfu": 0.026083673022474406, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 63, "data": {"grad_norm": 1.7003765106201172, "train/loss": 0.5114918947219849, "train/lr": 3.1500000000000003e-06, "train/mfu": 0.025358771162176268, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 64, "data": {"grad_norm": 1.622968077659607, "train/loss": 0.4892365038394928, "train/lr": 3.2000000000000003e-06, "train/mfu": 0.025574507690031334, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 65, "data": {"grad_norm": 1.710767388343811, "train/loss": 0.46010997891426086, "train/lr": 3.2500000000000002e-06, "train/mfu": 0.026044547488295888, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 66, "data": {"grad_norm": 1.5309820175170898, "train/loss": 0.4763348698616028, "train/lr": 3.3000000000000006e-06, "train/mfu": 0.02551705539547805, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 67, "data": {"grad_norm": 1.5878081321716309, "train/loss": 0.463325172662735, "train/lr": 3.3500000000000005e-06, "train/mfu": 0.02545574756459645, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 68, "data": {"grad_norm": 1.6647447347640991, "train/loss": 0.49605515599250793, "train/lr": 3.4000000000000005e-06, "train/mfu": 0.02576732988395891, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 69, "data": {"grad_norm": 1.547182321548462, "train/loss": 0.48861265182495117, "train/lr": 3.45e-06, "train/mfu": 0.025570731760732424, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 70, "data": {"grad_norm": 1.5329535007476807, "train/loss": 0.45033764839172363, "train/lr": 3.5e-06, "train/mfu": 0.025488252055198352, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 71, "data": {"grad_norm": 1.613761067390442, "train/loss": 0.4711205065250397, "train/lr": 3.5500000000000003e-06, "train/mfu": 0.025982561710374715, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 72, "data": {"grad_norm": 1.6405706405639648, "train/loss": 0.4805890619754791, "train/lr": 3.6000000000000003e-06, "train/mfu": 0.025596113078764696, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 73, "data": {"grad_norm": 1.6579740047454834, "train/loss": 0.46210306882858276, "train/lr": 3.65e-06, "train/mfu": 0.025542520806441246, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 74, "data": {"grad_norm": 1.5622249841690063, "train/loss": 0.45509427785873413, "train/lr": 3.7e-06, "train/mfu": 0.026102950699733188, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 75, "data": {"grad_norm": 1.5779720544815063, "train/loss": 0.4632663130760193, "train/lr": 3.7500000000000005e-06, "train/mfu": 0.02615288113626336, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 76, "data": {"grad_norm": 1.6686867475509644, "train/loss": 0.4986804723739624, "train/lr": 3.8000000000000005e-06, "train/mfu": 0.025719011165580948, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 77, "data": {"grad_norm": 1.4416990280151367, "train/loss": 0.43049538135528564, "train/lr": 3.85e-06, "train/mfu": 0.025902462548655223, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 78, "data": {"grad_norm": 1.496603012084961, "train/loss": 0.44676142930984497, "train/lr": 3.900000000000001e-06, "train/mfu": 0.025930273385282684, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 79, "data": {"grad_norm": 1.6020632982254028, "train/loss": 0.4762985408306122, "train/lr": 3.95e-06, "train/mfu": 0.02590681347718562, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 80, "data": {"grad_norm": 1.5295345783233643, "train/loss": 0.4706512689590454, "train/lr": 4.000000000000001e-06, "train/mfu": 0.025829848228135544, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 81, "data": {"grad_norm": 1.5433586835861206, "train/loss": 0.49515268206596375, "train/lr": 4.05e-06, "train/mfu": 0.025983652958619555, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 82, "data": {"grad_norm": 1.635560393333435, "train/loss": 0.4679352641105652, "train/lr": 4.1e-06, "train/mfu": 0.025407586601553577, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 83, "data": {"grad_norm": 1.4965288639068604, "train/loss": 0.4469429850578308, "train/lr": 4.15e-06, "train/mfu": 0.02586615992190186, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 84, "data": {"grad_norm": 1.4666553735733032, "train/loss": 0.4509005844593048, "train/lr": 4.2000000000000004e-06, "train/mfu": 0.026233034239033608, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 85, "data": {"grad_norm": 1.780860424041748, "train/loss": 0.4898082911968231, "train/lr": 4.25e-06, "train/mfu": 0.025897493967624224, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 86, "data": {"grad_norm": 1.5546119213104248, "train/loss": 0.4629536271095276, "train/lr": 4.3e-06, "train/mfu": 0.0259731433409679, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 87, "data": {"grad_norm": 1.5878740549087524, "train/loss": 0.45064371824264526, "train/lr": 4.350000000000001e-06, "train/mfu": 0.02590097094616921, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 88, "data": {"grad_norm": 1.8246179819107056, "train/loss": 0.46757638454437256, "train/lr": 4.4e-06, "train/mfu": 0.02621925125354971, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 89, "data": {"grad_norm": 1.6413278579711914, "train/loss": 0.4799474775791168, "train/lr": 4.450000000000001e-06, "train/mfu": 0.025983421886333256, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 90, "data": {"grad_norm": 1.649965524673462, "train/loss": 0.47140735387802124, "train/lr": 4.5e-06, "train/mfu": 0.026136960473606436, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 91, "data": {"grad_norm": 1.573211431503296, "train/loss": 0.4413219690322876, "train/lr": 4.5500000000000005e-06, "train/mfu": 0.026479700634252454, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 92, "data": {"grad_norm": 1.5877832174301147, "train/loss": 0.46922433376312256, "train/lr": 4.600000000000001e-06, "train/mfu": 0.026357524670412934, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 93, "data": {"grad_norm": 1.493066668510437, "train/loss": 0.47205525636672974, "train/lr": 4.65e-06, "train/mfu": 0.025900985662367027, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 94, "data": {"grad_norm": 1.4434045553207397, "train/loss": 0.45619645714759827, "train/lr": 4.7e-06, "train/mfu": 0.024975612075149287, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 95, "data": {"grad_norm": 1.5500119924545288, "train/loss": 0.4706301689147949, "train/lr": 4.75e-06, "train/mfu": 0.022135305748356555, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 96, "data": {"grad_norm": 1.4467713832855225, "train/loss": 0.4720730185508728, "train/lr": 4.800000000000001e-06, "train/mfu": 0.015421039962304131, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 97, "data": {"grad_norm": 1.5636159181594849, "train/loss": 0.47319522500038147, "train/lr": 4.85e-06, "train/mfu": 0.012637570072450593, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 98, "data": {"grad_norm": 1.6194660663604736, "train/loss": 0.42339032888412476, "train/lr": 4.9000000000000005e-06, "train/mfu": 0.012708220911497967, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 99, "data": {"grad_norm": 1.4967498779296875, "train/loss": 0.49978137016296387, "train/lr": 4.95e-06, "train/mfu": 0.012801977708333098, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 100, "data": {"grad_norm": 1.4705628156661987, "train/loss": 0.4505707621574402, "train/lr": 5e-06, "train/mfu": 0.012966685847577857, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 101, "data": {"grad_norm": 1.4164727926254272, "train/loss": 0.4248339831829071, "train/lr": 5.050000000000001e-06, "train/mfu": 0.012894357065644901, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 102, "data": {"grad_norm": 1.6551440954208374, "train/loss": 0.5030774474143982, "train/lr": 5.1e-06, "train/mfu": 0.0010547622561840843, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 103, "data": {"grad_norm": 1.4860306978225708, "train/loss": 0.4482958912849426, "train/lr": 5.150000000000001e-06, "train/mfu": 0.000606138636362198, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 104, "data": {"grad_norm": 1.4481829404830933, "train/loss": 0.4531690776348114, "train/lr": 5.2e-06, "train/mfu": 0.012677040942158256, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 105, "data": {"grad_norm": 1.3833285570144653, "train/loss": 0.4299805760383606, "train/lr": 5.2500000000000006e-06, "train/mfu": 0.012468256916191647, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 106, "data": {"grad_norm": 1.4534680843353271, "train/loss": 0.43930527567863464, "train/lr": 5.300000000000001e-06, "train/mfu": 0.012662416442000965, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 107, "data": {"grad_norm": 1.497080683708191, "train/loss": 0.4481316804885864, "train/lr": 5.3500000000000004e-06, "train/mfu": 0.012811622769881029, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 108, "data": {"grad_norm": 1.500756025314331, "train/loss": 0.4635733962059021, "train/lr": 5.400000000000001e-06, "train/mfu": 0.012334963661280547, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 109, "data": {"grad_norm": 1.4299366474151611, "train/loss": 0.43027061223983765, "train/lr": 5.450000000000001e-06, "train/mfu": 0.0019796832025382794, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 110, "data": {"grad_norm": 1.4930638074874878, "train/loss": 0.45603808760643005, "train/lr": 5.500000000000001e-06, "train/mfu": 0.0010745863690282454, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 111, "data": {"grad_norm": 1.5183780193328857, "train/loss": 0.4376876950263977, "train/lr": 5.550000000000001e-06, "train/mfu": 0.0010327992473864941, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 112, "data": {"grad_norm": 1.4703141450881958, "train/loss": 0.44168123602867126, "train/lr": 5.600000000000001e-06, "train/mfu": 0.0019400396802972558, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 113, "data": {"grad_norm": 1.3030767440795898, "train/loss": 0.41596347093582153, "train/lr": 5.65e-06, "train/mfu": 0.001080392170154479, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 114, "data": {"grad_norm": 1.497219443321228, "train/loss": 0.47532525658607483, "train/lr": 5.7e-06, "train/mfu": 0.0010775090303726195, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 115, "data": {"grad_norm": 1.5276576280593872, "train/loss": 0.4748345613479614, "train/lr": 5.75e-06, "train/mfu": 0.001992022932608582, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 116, "data": {"grad_norm": 1.494938850402832, "train/loss": 0.4847498834133148, "train/lr": 5.8e-06, "train/mfu": 0.0010772556791997232, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 116, "data": {"val/loss": 0.467637300491333}}
