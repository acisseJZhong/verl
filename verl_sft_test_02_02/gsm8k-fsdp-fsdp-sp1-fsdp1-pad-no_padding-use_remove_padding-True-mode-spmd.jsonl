{"step": 1, "data": {"grad_norm": 22.61558723449707, "train/loss": 1.371539831161499, "train/lr": 5.0000000000000004e-08, "train/mfu": 0.011275402020603039, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 2, "data": {"grad_norm": 22.199386596679688, "train/loss": 1.4289494752883911, "train/lr": 1.0000000000000001e-07, "train/mfu": 0.01488368301374798, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 3, "data": {"grad_norm": 21.36765480041504, "train/loss": 1.3436613082885742, "train/lr": 1.5000000000000002e-07, "train/mfu": 0.016288657112736245, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 4, "data": {"grad_norm": 21.606639862060547, "train/loss": 1.3845096826553345, "train/lr": 2.0000000000000002e-07, "train/mfu": 0.016232943945247765, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 5, "data": {"grad_norm": 22.879257202148438, "train/loss": 1.4229539632797241, "train/lr": 2.5000000000000004e-07, "train/mfu": 0.016966985863129898, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 6, "data": {"grad_norm": 20.754179000854492, "train/loss": 1.3689957857131958, "train/lr": 3.0000000000000004e-07, "train/mfu": 0.016947239859087655, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 7, "data": {"grad_norm": 22.431358337402344, "train/loss": 1.3783915042877197, "train/lr": 3.5000000000000004e-07, "train/mfu": 0.01701967432265275, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 8, "data": {"grad_norm": 22.70256233215332, "train/loss": 1.3690714836120605, "train/lr": 4.0000000000000003e-07, "train/mfu": 0.016605467750227438, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 9, "data": {"grad_norm": 22.213457107543945, "train/loss": 1.3349703550338745, "train/lr": 4.5000000000000003e-07, "train/mfu": 0.016796705409708318, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 10, "data": {"grad_norm": 23.184053421020508, "train/loss": 1.4386394023895264, "train/lr": 5.000000000000001e-07, "train/mfu": 0.01650665850783713, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 11, "data": {"grad_norm": 20.96369743347168, "train/loss": 1.3924546241760254, "train/lr": 5.5e-07, "train/mfu": 0.016817021341242754, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 12, "data": {"grad_norm": 21.387802124023438, "train/loss": 1.3794167041778564, "train/lr": 6.000000000000001e-07, "train/mfu": 0.016621254560556086, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 13, "data": {"grad_norm": 20.85187339782715, "train/loss": 1.3608018159866333, "train/lr": 6.5e-07, "train/mfu": 0.01730516964398818, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 14, "data": {"grad_norm": 19.549884796142578, "train/loss": 1.3530641794204712, "train/lr": 7.000000000000001e-07, "train/mfu": 0.017247758284814098, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 15, "data": {"grad_norm": 19.57367706298828, "train/loss": 1.3270225524902344, "train/lr": 7.5e-07, "train/mfu": 0.01723162309105362, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 16, "data": {"grad_norm": 17.36261558532715, "train/loss": 1.2707514762878418, "train/lr": 8.000000000000001e-07, "train/mfu": 0.016864931697190545, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 17, "data": {"grad_norm": 18.332233428955078, "train/loss": 1.3428906202316284, "train/lr": 8.500000000000001e-07, "train/mfu": 0.017154884700912424, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 18, "data": {"grad_norm": 17.297374725341797, "train/loss": 1.2850165367126465, "train/lr": 9.000000000000001e-07, "train/mfu": 0.017192630075265183, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 19, "data": {"grad_norm": 19.001976013183594, "train/loss": 1.2870270013809204, "train/lr": 9.500000000000001e-07, "train/mfu": 0.018474574136924736, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 20, "data": {"grad_norm": 16.248790740966797, "train/loss": 1.1788040399551392, "train/lr": 1.0000000000000002e-06, "train/mfu": 0.024983122330113346, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 21, "data": {"grad_norm": 16.396326065063477, "train/loss": 1.18631911277771, "train/lr": 1.0500000000000001e-06, "train/mfu": 0.002407385293931361, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 22, "data": {"grad_norm": 15.81033706665039, "train/loss": 1.1508519649505615, "train/lr": 1.1e-06, "train/mfu": 0.02442299594864498, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 23, "data": {"grad_norm": 16.214248657226562, "train/loss": 1.1233688592910767, "train/lr": 1.1500000000000002e-06, "train/mfu": 0.002372474988260036, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 24, "data": {"grad_norm": 16.433609008789062, "train/loss": 1.1242387294769287, "train/lr": 1.2000000000000002e-06, "train/mfu": 0.024675244001771617, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 25, "data": {"grad_norm": 19.047924041748047, "train/loss": 1.1642073392868042, "train/lr": 1.25e-06, "train/mfu": 0.0021881691186133906, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 26, "data": {"grad_norm": 17.702983856201172, "train/loss": 1.0591082572937012, "train/lr": 1.3e-06, "train/mfu": 0.02460268103175836, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 27, "data": {"grad_norm": 10.795517921447754, "train/loss": 0.9028133153915405, "train/lr": 1.3500000000000002e-06, "train/mfu": 0.0023354739532550767, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 28, "data": {"grad_norm": 10.145936965942383, "train/loss": 0.8744242787361145, "train/lr": 1.4000000000000001e-06, "train/mfu": 0.022886680642504705, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 29, "data": {"grad_norm": 9.864659309387207, "train/loss": 0.8672052025794983, "train/lr": 1.45e-06, "train/mfu": 0.0023621689405021375, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 30, "data": {"grad_norm": 10.108447074890137, "train/loss": 0.8454594612121582, "train/lr": 1.5e-06, "train/mfu": 0.02419864361824207, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 31, "data": {"grad_norm": 9.280369758605957, "train/loss": 0.795650839805603, "train/lr": 1.5500000000000002e-06, "train/mfu": 0.0023599080874987244, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 32, "data": {"grad_norm": 8.912195205688477, "train/loss": 0.8301644921302795, "train/lr": 1.6000000000000001e-06, "train/mfu": 0.020927934322951185, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 33, "data": {"grad_norm": 8.737667083740234, "train/loss": 0.8436235189437866, "train/lr": 1.6500000000000003e-06, "train/mfu": 0.0022748840682971666, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 34, "data": {"grad_norm": 8.464752197265625, "train/loss": 0.7490852475166321, "train/lr": 1.7000000000000002e-06, "train/mfu": 0.02033512921293085, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 35, "data": {"grad_norm": 7.469737529754639, "train/loss": 0.7242494821548462, "train/lr": 1.75e-06, "train/mfu": 0.0023505068988229046, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 36, "data": {"grad_norm": 6.274004936218262, "train/loss": 0.7256113290786743, "train/lr": 1.8000000000000001e-06, "train/mfu": 0.019706974411113957, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 37, "data": {"grad_norm": 5.6860809326171875, "train/loss": 0.6604232788085938, "train/lr": 1.85e-06, "train/mfu": 0.024305988141373466, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 38, "data": {"grad_norm": 5.80422830581665, "train/loss": 0.668536901473999, "train/lr": 1.9000000000000002e-06, "train/mfu": 0.00224240882984619, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 39, "data": {"grad_norm": 6.114563941955566, "train/loss": 0.6410922408103943, "train/lr": 1.9500000000000004e-06, "train/mfu": 0.024156217944287267, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
