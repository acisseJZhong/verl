## Debug Comparison Guide

Add these debug prints to compare FSDP vs TorchTitan at each step:

### For TorchTitan (in forward_step, around line 970-1040):

```python
if torch.distributed.get_rank() == 0:
    model = self.trainer.model_parts[0]
    with torch.no_grad():
        input_ids = model_inputs["input_ids"]
        positions = model_inputs["position_ids"]

        # 1. Embedding
        embed = model.tok_embeddings(input_ids)

        # 2. First layer
        layer0 = model.layers["0"]
        attn_norm_out = layer0.attention_norm(embed)

        # 3. Q/K projections
        xq = layer0.attention.wq(attn_norm_out)
        xk = layer0.attention.wk(attn_norm_out)

        # Print single line summary
        print(f"TITAN_SUMMARY: embed[{embed.mean():.6f},{embed.std():.6f}] "
              f"attn_norm[{attn_norm_out.mean():.6f},{attn_norm_out.std():.6f}] "
              f"Q[{xq.mean():.6f},{xq.std():.6f}] "
              f"K[{xk.mean():.6f},{xk.std():.6f}] "
              f"logits[{logits.mean():.6f},{logits.std():.6f}]")
```

### For FSDP (in forward_step, after model forward):

```python
if torch.distributed.get_rank() == 0:
    with torch.no_grad():
        input_ids = model_inputs["input_ids"]

        # 1. Embedding
        embed = self.module.model.embed_tokens(input_ids)

        # 2. First layer
        layer0 = self.module.model.layers[0]
        attn_norm_out = layer0.input_layernorm(embed)

        # 3. Q/K projections
        xq = layer0.self_attn.q_proj(attn_norm_out)
        xk = layer0.self_attn.k_proj(attn_norm_out)

        # Print single line summary
        print(f"FSDP_SUMMARY: embed[{embed.mean():.6f},{embed.std():.6f}] "
              f"attn_norm[{attn_norm_out.mean():.6f},{attn_norm_out.std():.6f}] "
              f"Q[{xq.mean():.6f},{xq.std():.6f}] "
              f"K[{xk.mean():.6f},{xk.std():.6f}] "
              f"logits[{logits.mean():.6f},{logits.std():.6f}]")
```

Then compare the output:
```
FSDP_SUMMARY: embed[0.001234,0.456789] attn_norm[...] Q[...] K[...] logits[...]
TITAN_SUMMARY: embed[0.001234,0.456789] attn_norm[...] Q[...] K[...] logits[...]
```

If embedding differs → weight loading issue
If attn_norm differs → RMSNorm patch issue
If Q/K differs → projection weight issue
If logits differ but earlier stages match → attention/FFN issue
